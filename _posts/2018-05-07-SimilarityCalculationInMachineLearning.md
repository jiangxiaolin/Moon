---
layout: post  
title: "机器学习常见相似度计算公式总结"  
date: 2018-05-07  
excerpt: "常见相似计算公式，如余弦公式，欧式距离，皮尔逊相关系数"  
tags: [相似度，距离]
comments: true
---


 在自然语言处理和推荐系统等问题中，相似度计算是一个非常重要的方面，因此本文总结了常用的相似度或距离计算公式便于随时查看。

假设我们有两个向量 X=[X1, …,Xn] 和 Y=[Y1, …,Yn] ，长度均为 n

**欧氏距离**：m维空间中两个点之间的真实距离

$$d(X,Y)=\sqrt { \sum _{ i=1 }^{ n }{ { ({ X }_{ i }-{ Y }_{ i }) }^{ 2 } }  } $$

**Pearson相关性系数**：反映两个变量线性相关程度的统计量

$$COR(X,Y)=\frac { \sum _{ i=1 }^{ n }{ { ({ X }_{ i }-\bar { X } ) }^{ 2 } } \sum _{ i=1 }^{ n }{ { (Y_{ i }-\bar { Y } ) }^{ 2 } }  }{ \sqrt { \sum _{ i=1 }^{ n }{ { ({ X }_{ i }-\bar { X } ) }^{ 2 } }  } \sqrt { \sum _{ i=1 }^{ n }{ { (Y_{ i }-\bar { Y } ) }^{ 2 } }  }  } $$


**余弦相似度**：空间两个向量夹角之间的余弦值

$$COS(X,Y)=\frac { \sum _{ i=1 }^{ n }{ { ({ X }_{ i }) }^{ 2 } } \sum _{ i=1 }^{ n }{ { (Y_{ i }) }^{ 2 } }  }{ \sqrt { \sum _{ i=1 }^{ n }{ { ({ X }_{ i }) }^{ 2 } }  } \sqrt { \sum _{ i=1 }^{ n }{ { (Y_{ i }) }^{ 2 } }  }  } $$

**曼哈顿距离**：曼哈顿距离又称马氏距离 每个维度差值绝对值的累加和

$$Manhattan(X,Y)=\quad \sum _{ i=1 }^{ n }{ \left| { X }_{ i }-{ Y }_{ i } \right|  } $$

**切比雪夫距离**：计算每个维度差绝对值  取其中最大的

$$QBXF(X,Y)=\quad max(\left| { X }_{ i }-{ Y }_{ i } \right| )$$


**Jaccard相似度**: 样本集交集与样本集合集的比值
$$Jaccard（A, B）= |A intersectB| / |A union B|$$

**广义Jaccard相似度 / Tanimoto 系数**:  由Jaccard系数和余弦公式扩展而来

$$Tanimoto(X,Y)=\quad \frac { \sum _{ i=1 }^{ n }{ { X }_{ i }{ Y }_{ i } }  }{ \sqrt { \sum _{ i=1 }^{ n }{ { { X }_{ i }^{ 2 } } }  } \sqrt { \sum _{ i=1 }^{ n }{ { Y_{ i }^{ 2 } } }  } -\sum _{ i=1 }^{ n }{ { X }_{ i }{ Y }_{ i } }  } $$

**spearman相关系数**：衡量两个变量的依赖性的 非参数 指标。 它利用单调方程评价两个统计变量的相关性。 如果数据中没有重复值， 并且当两个变量完全单调相关时，斯皮尔曼相关系数则为+1或−1

$$Spearman(X,Y)=\quad 1-\frac { 6\sum _{ i=1 }^{ n }{ { d }_{ i }^{ 2 } }  }{ n({ n }^{ 2 }-1) } $$

其中d为每个维度秩的差值，秩的定义为：每个维度上，将该维度所有数进行从大到小的排序，每个数在排序后列表中所处的位置为秩  d就是Xi和Yi的秩的差值




